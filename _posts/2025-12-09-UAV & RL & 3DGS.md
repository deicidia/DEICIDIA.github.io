---
layout: post
title: Reconstruction 3D Autonome
date: 2025-12-09
categories: [Robotics, RL, 3DGS]
tags: [UAV, Computer Vision, Optimization]
---

Ce projet vise à optimiser la trajectoire de vol d’un drone via l'Apprentissage par Renforcement (RL) afin d'améliorer la qualité et la vitesse d'une reconstruction 3D basée sur le **3D Gaussian Splatting (3DGS)**.

L'idée centrale repose sur l'efficacité : au lieu de scanner naïvement une zone entière (pattern "tondeuse à gazon"), le drone utilise un signal d’incertitude fourni par le modèle pour cibler les zones mal reconstruites. C'est le principe du **Next-Best-View (NBV) Planning**. L’objectif est de maximiser le gain d'information tout en minimisant le temps de vol.

<figure style="text-align: center;">
    <img src="{{ site.baseurl }}/images/prometheus.gif" alt="Drone scanning simulation" width="100%" style="border-radius: 8px;">
    <figcaption><i>Je fixe une caméra 360° sur un drone et le tour est joué, probablement</i></figcaption>
</figure>

---

## Architecture du Pipeline

Pour atteindre cette autonomie sur matériel embarqué, le système doit fonctionner en boucle fermée. Voici l'architecture "Perception-Action" envisagée :

<figure style="text-align: center;">
    <img src="{{ site.baseurl }}/images/UAV.png" alt="Architecture UAV RL" width="90%" style="display: block; margin: 0 auto;">
</figure>

1.  **Acquisition :** Le drone capture le flux vidéo et les données inertielles (IMU/GPS).
2.  **Estimation & Reconstruction :** Un modèle 3DGS est initialisé et mis à jour incrémentalement.
3.  **Analyse d'incertitude :** Le système identifie les zones de "flou" ou de faible densité de gaussiennes (l'axe de recherche principal).
4.  **Décision (RL) :** L'agent reçoit cet état (carte de densité + position) et calcule le prochain point de vue optimal.

---

## État des lieux et Benchmarks

Le défi majeur du *Real-Time RL* est la latence. Avant d'intégrer le contrôle, il est crucial de valider la chaîne de reconstruction et d'identifier les goulots d'étranglement.

### 1. Baseline "Grand Public" (Smartphone)
J'ai établi une référence de qualité/vitesse avec une approche mobile.
* **Sujet :** [Panda roux](/scans/panda-roux.html)
* **Outils :** [Scaniverse](https://scaniverse.com/) (capture) & [SuperSplat](https://developer.playcanvas.com/user-manual/) (export).
* **Constat :** Rapide, mais dépendant du traitement cloud ou de l'optimisation iOS propriétaire.

### 2. Haute fidélité : Fujifilm XT-2 + COLMAP
Pour simuler un drone porteur d'une optique de qualité, j'ai utilisé un Fuji XT-2 (23mm f/2, eq. 35mm). Le traitement a été réalisé sur PC (RX 7800XT).

<figure style="text-align: center;">
    <img src="/images/zig.png" alt="Visualisation Zig" width="100%" style="border-radius: 8px;">
    <figcaption><i>Scan test : "Zigoto" ou "Le Zig"</i></figcaption>
</figure>

#### Comparatif des méthodes de Matching
La lourdeur du calcul *Structure-from-Motion* (SfM) est l'obstacle principal. 

Voici les résultats comparatifs pour une extraction de features, feature matching et reconstruction:

| Méthode | Source | Temps de calcul | Qualité |
| :--- | :--- | :--- | :--- | :--- |
| **Exhaustive Matching** | 30 Photos | ~10 min | Élevée |
| **Sequential Matching** | Vidéo | **~5 min** | Moyenne |

Scan du "Zig" par vidéo ([voir le modèle ici](/scans/zigoto.html)).

#### Contraintes Matérielles
* **Optique :** Le 23mm (eq. 35mm) est en limite basse pour la photogrammétrie (recommandation : 50-85mm). Il capte beaucoup d'environnement inutile.
* **Stabilisation :** L'absence de stabilisation du capteur (IBIS) sur le XT-2 et l'objectif génère du flou de bougé, fatal pour la reconstruction 3DGS.

### 3. Le "Mur" du passage à l'échelle
Une tentative de scan complet de mon appartement a échoué.
* **Windows :** COLMAP a crashé (trop de features, gestion mémoire défaillante).
* **WSL2 + Brush :** Seules 23 images conservées sur plusieurs centaines.
* **Leçon :** Sans un GPU NVIDIA (CUDA) robuste et une gestion mémoire stricte, le pipeline classique COLMAP ne passe pas l'échelle.

---

## Pistes d'optimisation : Contourner le SfM

L'analyse est claire : **le goulot d'étranglement n'est pas l'entraînement des gaussiennes, mais le calcul de pose (SfM).**

Comme souligné dans cette discussion [Reddit](https://www.reddit.com/r/GaussianSplatting/comments/1mdjscn/how_is_the_scaniverse_app_even_possible/) :
> "La plupart du temps, créer un 3DGS revient à faire du Structure from Motion. Si vous connaissez l'ordre des images [...] et récupérez les données du gyroscope, vous pouvez éliminer la majeure partie du temps de SfM."

Le drone disposant de capteurs précis (IMU, Odométrie, GPS), nous pouvons :
1.  Initialiser les poses avec la télémétrie.
2.  Utiliser un **SLAM visuel embarqué** pour remplacer COLMAP.

### Solutions techniques à l'étude

J'étudie actuellement des alternatives "SfM-Free" ou optimisées :

#### 1. [GLOMAP](https://github.com/colmap/glomap)
Une alternative moderne à COLMAP pour le mapping global. Je suis bloqué temporairement par un [bug connu](https://github.com/colmap/glomap/issues/221). Nécessite toujours une étape de feature matching préalable.

#### 2. [CF-3DGS](https://github.com/NVlabs/CF-3DGS) 

En cours de test.